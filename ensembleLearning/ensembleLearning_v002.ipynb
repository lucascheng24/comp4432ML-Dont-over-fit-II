{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ensemble Learning</h1>\n",
    "# setting training set labels to 50/50 %\n",
    "    <p>160 class 0 samples by adding 70 class 0 records\n",
    "it should adjust the features of the added function\n",
    "<br> <ul>adjust the added sample with percentage adjustment</ul>\n",
    "<h4>score: private 0.638 ; public 0.674</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>step1: <b>Pre-process training data</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def randomInteger(a, b):\n",
    "    return random.randint(a, b)\n",
    "\n",
    "def adjustValue(value, percentageValue):    # 0 to xx.x%\n",
    "    return_value = value\n",
    "    plusORminus = randomInteger(0, 1) # 0 is plus, 1 is minus\n",
    "\n",
    "    if (plusORminus == 0):\n",
    "        return_value += value*(randomInteger(0, percentageValue)/1000)\n",
    "    else:\n",
    "        return_value -= value*(randomInteger(0, percentageValue)/1000)\n",
    "\n",
    "    if (np.isnan(return_value)):\n",
    "        print(\"NaN - value*(randomInteger(0, percentageValue)/1000): \", value*(randomInteger(0, percentageValue)/1000))\n",
    "    \n",
    "    return return_value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 : 90\n",
      "160 : 160\n",
      "Number of NaN values present: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# df = pd.read_csv('./train.csv', index_col = True)\n",
    "url = 'https://raw.githubusercontent.com/lucascheng24/comp4432ML-Dont-over-fit-II/main/train.csv'\n",
    "df = pd.read_csv(url, index_col=\"id\")\n",
    "df['target'] = df['target'].astype(int)\n",
    "\n",
    "parameters_dictionary = {}\n",
    "\n",
    "df_class_1 = df[df['target'] == 1]\n",
    "df_class_0 = df[df['target'] == 0]\n",
    "\n",
    "adjusted_percentage_range = 5.5 # best result is 5.5%\n",
    "\n",
    "\n",
    "print(len(df_class_1), \":\", len(df_class_0))\n",
    "\n",
    "more_class_0 = df_class_0.sample(n=70)\n",
    "more_class_0 = more_class_0.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# random the new class(0) data\n",
    "index = 0\n",
    "feature_index = 0\n",
    "\n",
    "for i in range(len(more_class_0['target'])):\n",
    "    # print(row)\n",
    "    for j in range(300):\n",
    "        more_class_0.at[i, str(j)] = adjustValue(more_class_0.at[i, str(j)], adjusted_percentage_range*10)\n",
    "        # if (np.isnan(more_class_0.at[i, str(j)])):\n",
    "        #     print(\"NaN index;column :\", i, \";\", j)\n",
    "        # item[str(i)] = adjustValue(item[str(i)], 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # index = randomInteger(0, 159)\n",
    "    # feature_index = randomInteger(0, 299)\n",
    "\n",
    "\n",
    "more_class_0 = more_class_0.drop(columns=['id'])\n",
    "\n",
    "df_class_0 = pd.concat([df_class_0, more_class_0])\n",
    "\n",
    "# df_class_0 = more_class_0.reset_index()\n",
    "\n",
    "\n",
    "df = pd.concat([df_class_0, df_class_1])\n",
    "\n",
    "# df = df.reset_index()\n",
    "\n",
    "# print(df)\n",
    "\n",
    "df.reset_index()\n",
    "\n",
    "df.rename(columns = {'index': 'id'})\n",
    "\n",
    "print(len(df_class_1), \":\", len(df_class_0))\n",
    "\n",
    "for element in df:\n",
    "    if element != \"target\":\n",
    "        parameters_dictionary.update({element : df[element]})\n",
    "\n",
    "\n",
    "\n",
    "label_dictionary = {'label': df['target']}\n",
    "\n",
    "df_X = pd.DataFrame(data = parameters_dictionary)\n",
    "df_Y = pd.DataFrame(data = label_dictionary)\n",
    "\n",
    "# print(df_X['id'])\n",
    "\n",
    "# applying the method\n",
    "nan_in_df = df_X.isnull().sum().sum()\n",
    " \n",
    "# printing the number of values present in\n",
    "# the whole dataframe\n",
    "print('Number of NaN values present: ' + str(nan_in_df))\n",
    "# print(df_class_0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'estimator' parameter of BaggingClassifier must be an object implementing 'fit' and 'predict' or None. Got [LogisticRegression(), DecisionTreeClassifier(), SVC(kernel='linear')] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m svm_clf \u001b[39m=\u001b[39m SVC(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Linear Kernel\u001b[39;00m\n\u001b[0;32m     11\u001b[0m baggingClassifier \u001b[39m=\u001b[39m ensemble\u001b[39m.\u001b[39mBaggingClassifier(estimator \u001b[39m=\u001b[39m [LogisticRegression(), dt_clf, svm_clf], \n\u001b[0;32m     12\u001b[0m                                                n_estimators \u001b[39m=\u001b[39m \u001b[39m101\u001b[39m, max_features\u001b[39m=\u001b[39m\u001b[39m250\u001b[39m, max_samples\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m\n\u001b[0;32m     13\u001b[0m                                                )\n\u001b[1;32m---> 14\u001b[0m bagging_model \u001b[39m=\u001b[39m baggingClassifier\u001b[39m.\u001b[39;49mfit(df_X, df_Y)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:326\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    303\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a Bagging ensemble of estimators from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[0;32m    328\u001b[0m     \u001b[39m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    330\u001b[0m         X,\n\u001b[0;32m    331\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    335\u001b[0m         multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    336\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    574\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \n\u001b[0;32m    576\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    582\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[0;32m    583\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    584\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[0;32m    585\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     98\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'estimator' parameter of BaggingClassifier must be an object implementing 'fit' and 'predict' or None. Got [LogisticRegression(), DecisionTreeClassifier(), SVC(kernel='linear')] instead."
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble, preprocessing, metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# initialize the base classifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "#Create a svm Classifier\n",
    "svm_clf = SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "baggingClassifier = ensemble.BaggingClassifier(estimator = [LogisticRegression(), dt_clf, svm_clf], \n",
    "                                               n_estimators = 101, max_features=250, max_samples=200\n",
    "                                               )\n",
    "bagging_model = baggingClassifier.fit(df_X, df_Y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement logistic regression\n",
    "\n",
    "\n",
    "###\n",
    "#   function: predict_logistic_regression\n",
    "#       @param: input_x => training dataset features\n",
    "#       @param: input_y => training dataset label\n",
    "#       @param: input_z => testing dataset records (features only)\n",
    "#   \n",
    "#   output: array/list of predicted result of input_z\n",
    "###\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "def predict_logistic_regression(input_x, input_y, input_z):\n",
    "    # build the model based on training data\n",
    "    model.fit(input_x, input_y)\n",
    "\n",
    "    # insert testing dataset to get the predicted output\n",
    "    predicted_values = model.predict(input_z)\n",
    "\n",
    "    return predicted_values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Applying 5-fold-cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits = k, random_state = None)\n",
    "\n",
    "\n",
    "# A function to run 5-fold cross validation on polynomial_logistic_regression\n",
    "def kFold_logistic_regression(input_X, input_Y):\n",
    "\n",
    "    validation_f1_score = []\n",
    "\n",
    "    for train_index , validation_index in kf.split(input_X):\n",
    "\n",
    "        # print('Start model fitting, k=', train_index[0], ' - ', train_index[-1], ' ...')\n",
    "\n",
    "        X_train , X_valid = input_X.iloc[train_index,:], input_X.iloc[validation_index,:]\n",
    "        y_train, y_valid = input_Y.iloc[train_index], input_Y.iloc[validation_index]\n",
    "\n",
    "        # print('X_train: ', X_train)\n",
    "\n",
    "        y_train = y_train.to_numpy().ravel()\n",
    "        y_valid = y_valid.to_numpy().ravel()\n",
    "        \n",
    "        pred_testValues = predict_logistic_regression(X_train, y_train, X_valid)\n",
    "\n",
    "        # count the f1 score(true records / predicted records)\n",
    "        valid_f1 = f1_score(y_valid, pred_testValues)\n",
    "        validation_f1_score.append(valid_f1)\n",
    "\n",
    "    return validation_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1 = []\n",
    "\n",
    "testing_f1_score = kFold_logistic_regression(df_X, df_Y)\n",
    "\n",
    "\n",
    "# print('testing set F1 score (poly 2 to 12) under 5-fold-cross-validation: ')\n",
    "for item in testing_f1_score:\n",
    "    print(item)\n",
    "    if item != 0.0:\n",
    "        mean_f1.append(item)\n",
    "    \n",
    "np.mean(mean_f1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Real prediction on testing data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filePath: ../result_folder/ensembleLearning_v1_1.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "url_test = 'https://raw.githubusercontent.com/lucascheng24/comp4432ML-Dont-over-fit-II/main/test.csv'\n",
    "df_test = pd.read_csv(url_test, index_col=\"id\")\n",
    "\n",
    "# print(df_test)\n",
    "\n",
    "# pred_values = predict_logistic_regression(df_X, df_Y, df_test)\n",
    "pred_values = bagging_model.predict(df_test)\n",
    "\n",
    "filePath = '../result_folder/ensembleLearning_v1_1.csv'\n",
    "\n",
    "\n",
    "print(\"filePath:\", filePath)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filePath, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"target\"])\n",
    "    for i in range(250, 20000):\n",
    "        writer.writerow([i, pred_values[i-250]])\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
